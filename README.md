# SafeVisionBench: A Hybrid Multi-Task Benchmark and Evaluation Framework for Safety and Unlearning in Text-to-Image Models
**WARNING: This repository contains model outputs that may be offensive in nature.**

## Introduction

SafeVisionBench is a comprehensive evaluation framework for assessing machine unlearning effectiveness, safety robustness, and cross-task generalization.

## Features

- Multi-Task Unlearning Evaluation: Supports assessment across three representative unlearning tasks, enabling precise detection of subtle differences in AI-generated images after concept erasure.
- Comprehensive Dataset Support: A large-scale, multi-source, and multi-task dataset that includes both real-world data and samples generated by multiple generative models, covering privacy-sensitive, harmful, and safety-critical unlearning scenarios, and designed to enable robust and realistic evaluation.
- Multi-Task Generalizable Classifier: A multi-head CLIP-based classifier designed to achieve strong cross-task and cross-domain generalization, enabling accurate and robust evaluation across diverse unlearning and safety scenarios.

---

## Installation Guide
The code base is based on the `diffusers` package. To get started:
```bash
git clone https://github.com/karrymeng-0921/SafeVisionBench.git
cd SafeVisionBench
pip install -r requirements.txt
```

## Training Guide
After installation, follow these instructions to train the SafeVisionClassifier:
```bash
python models/SafeVisionClassifier.py --base-dir '/path/to/your/dataset_root' --results-dir '/path/to/your/save_root' --tasks object style nsfw --batch-size 32
```

## Evaluate Guide
You can evaluate common unlearning results with the provided script `evaluate_unlearn.py`:
```bash
python evaluate/evaluate_unlearn.py --base-dir '/path/to/your/unlearn_images_root' -weight-dir '/path/to/your/Weightse_root' --save-dir '/path/to/your/save_root' --tasks 'object_church style_vangogh nsfw' --methods 'ESD FMN UCE MACE' --batch-size 32
```
### Arguments
- `--tasks`: The tasks to evaluate.  
  Supported:
  - `"Nudity"` (7 classes, see `utils/class_to_label.py` for task names)
  - `"Style"` (20 classes, see `utils/class_to_label.py` for task names)
  - `"Object"` (10 classes, see `utils/class_to_label.py` for task names)
- `--weight-dir`: Path to the classifier head weights, which can be your own trained weights (obtained after running `SafeVisionClassifier.py`) or the pre-trained weights we provide (our weights are located in the `Weights` directory).
- `--batch-size`: Batch size for evaluation.

## Generation Images
To use `evaluate/generate-images.py` you would need a CSV file with columns prompt, evaluation_seed, and case_number. You can use the following example to generate images with the standard diffusion model (e.g., SD-1.4, SD-1.5, SD-2.0 or SD-2.1).:
```bash
python evaluate/generate-images.py --model_name "None" --prompts_path "/path/to/your/prompts_csv" --save_path "/path/to/your/save_root" --base "1.4" --device "cuda:0" --guidance_scale 7.5 --image_size 512 --num_samples 1 --ddim_steps 100
```
You can use the following example to generate images after unlearning:
```bash
python evaluate/generate-images.py --model_name "ConceptPrune" --prompts_path "/path/to/your/prompts_csv" --save_path "/path/to/your/save_root" --base "1.4" --device "cuda:0" --guidance_scale 7.5 --image_size 512 --num_samples 1 --ddim_steps 100
```

## ⚠️ Data Availability Notice

Due to safety and compliance considerations, this repository **does not include** the complete dataset used in the experiments.  
Some data samples may contain unsafe or sensitive content and therefore cannot be publicly released. Partial sample examples are provided in the `dataset` directory.

We provide the model weights and evaluation code necessary to reproduce the experimental results.


