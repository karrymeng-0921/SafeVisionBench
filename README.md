# SafeVisionBench: A Hybrid Multi-Task Benchmark and Evaluation Framework for Safety and Unlearning in Text-to-Image Models
**WARNING: This repository contains model outputs that may be offensive in nature.**

## Introduction

SafeVisionBench is a comprehensive evaluation framework for assessing machine unlearning effectiveness, safety robustness, and cross-task generalization.

## Features

- Multi-Task Unlearning Evaluation: Supports assessment across three representative unlearning tasks, enabling precise detection of subtle differences in AI-generated images after concept erasure.
- Comprehensive Dataset Support: A large-scale, multi-source, and multi-task dataset that includes both real-world data and samples generated by multiple generative models, covering privacy-sensitive, harmful, and safety-critical unlearning scenarios, and designed to enable robust and realistic evaluation.
- Multi-Task Generalizable Classifier: A multi-head CLIP-based classifier designed to achieve strong cross-task and cross-domain generalization, enabling accurate and robust evaluation across diverse unlearning and safety scenarios.

---

## Installation Guide
The code base is based on the `diffusers package`. To get started:
```bash
git clone https://github.com/karrymeng-0921/SafeVisionBench.git
cd SafeVisionBench
pip install -r requirements.txt
```

## Training Guide
After installation, follow these instructions to train the SafeVisionClassifier:
```bash
python models/SafeVisionClassifier.py --base-dir '/path/to/your/dataset_root' --results-dir '/path/to/your/save_root' --batch-size 32
```

## Evaluate Guide
You can evaluate common unlearning results with the provided script `evaluate_unlearn.py`:
```bash
python evaluate_unlearn.py --base-dir '/path/to/your/unlearn_images_root' --model-dir '/path/to/your/Weightse_root' --save-dir '/path/to/your/save_root' --tasks 'object_church style_vangogh nsfw' --methods 'ESD FMN UCE MACE' --batch-size 32
```
### Arguments
- `--tasks`: The tasks to evaluate.  
  Supported:
  - `"Nudity"`
  - `"Style"` (129 classes, see `utils/name_to_id.py` for task names)
  - `"Object"` (10 classes, see `utils/name_to_id.py` for task names)
- `--batch-size`: Batch size for evaluation.

## Example Usage

You can evaluate your unlearning results with the provided script `eval_unlearning.py`.

```bash
python eval_unlearning.py --indicator multi_multiC --concept "Nudity" --batch-size 5
```

### Arguments
- `--concept`: The concept to evaluate.  
  Supported:
  - `"Nudity"`
  - `"Style"` (129 classes, see `utils/name_to_id.py` for task names)
  - `"Object"` (10 classes, see `utils/name_to_id.py` for task names)
- `--indicator`: Choose from:
  - `bi_multiC` (binary classification)
  - `multi_multiC` (multi-class classification)
- `--batch-size`: Batch size for evaluation.


## Getting Started (Benchmarking, 20250606)

```bash
# Create and activate the Conda environment
conda env create --file environment.yaml
conda activate igmu

# Run benchmarking
# We have included some demo images related to Nudity unlearning as examples. Please refer to "dataset/Benchmarking_images_demo"
python -W ignore benchmark.py --evaluation-aspect forgetting|fid|lpips|yolo|CSDR

# evaluate fid and lpips for 
# We have included some demo images related to Church unlearning as examples. Please refer to "dataset/Benchmarking_images_demo"
python -W ignore benchmark.py --object True --evaluation-aspect fid|lpips

```
### NOTE

By default, `/home/mrliu/miniconda3/envs/igmu/lib/python3.12/site-packages/torchmetrics/functional/multimodal/clip_score.py` returns the mean CLIP score for a batch.  
If you wish to obtain the individual CLIP scores for all images in a batch, please comment out line 165 in `clip_score.py`:

```python
model, processor = _get_clip_model_and_processor(model_name_or_path)
device = images.device if isinstance(images, Tensor) else images[0].device
score, _ = _clip_score_update(images, text, model.to(device), processor)
# score = score.mean(0) # line 165
return torch.max(score, torch.zeros_like(score))
```

## Citation

If you find this work helpful, we would greatly appreciate it if you could cite it.

```bibtex
@inproceedings{liu2025igmu,
  title = {Rethinking Machine Unlearning in Image Generation Models},
  author = {Liu, Renyang and Feng, Wenjie and Zhang, Tianwei and Zhou, Wei and Cheng, Xueqi and Ng, See-Kiong},
  booktitle = {ACM Conference on Computer and Communications Security (CCS)},
  organization = {ACM},
  year = {2025},
}

```

### Acknowledgements

We extend our gratitude to the following repositories for their contributions and resources:

- [ESD](https://github.com/rohitgandikota/erasing)
- [FMN](https://github.com/SHI-Labs/Forget-Me-Not)
- [SPM](https://github.com/Con6924/SPM)
- [AdvUnlearn](https://github.com/OPTML-Group/AdvUnlearn)
- [MACE](https://github.com/shilin-lu/mace)
- [RECE](https://github.com/CharlesGong12/RECE)
- [DoCo](https://github.com/yongliang-wu/DoCo)
- [Receler](https://github.com/jasper0314-huang/Receler)
- [ConceptPrune](https://github.com/ruchikachavhan/concept-prune)
- [UCE](https://github.com/rohitgandikota/unified-concept-editing)

- [NudeNet](https://github.com/notAI-tech/NudeNet)
- [UnlearnDiffAtk](https://github.com/OPTML-Group/Diffusion-MU-Attack)
- [IQA-PyTorch](https://github.com/chaofengc/IQA-PyTorch)


Their works have significantly contributed to the development of our work.
